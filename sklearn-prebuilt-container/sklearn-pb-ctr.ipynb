{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Originally modified from this example:<br>\n",
    " <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/unofficial/sdk/AI_Platform_(Unified)_SDK_Custom_Training_Python_Package_Managed_Text_Dataset_Tensorflow_Serving_Container.ipynb\">Vertex SDK for Python: Custom Training using Python Package, Managed Text Dataset, and TF-Serving Container Example</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you're looking for an example to build a custom container to serve a scikit-learn model on Vertex Predictions, check this:<br>\n",
    " <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/unofficial/sdk/AI_Platform_(Unified)_SDK_Custom_Container_Prediction.ipynb\">Vertex SDK for Python: Custom Container Prediction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Author: Jose Brache <br>\n",
    " Email: jbrache@google.com <br>\n",
    " <img src=\"img/google-cloud-icon.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertex SDK for Python: Custom Training using Python Package, and scikit-learn Container Example\n",
    "To use this Jupyter notebook, copy the notebook to a Google Cloud Notebooks instance with Tensorflow installed and open it. You can run each step, or cell, and see its results. To run a cell, use Shift+Enter. Jupyter automatically displays the return value of the last line in each cell. For more information about running notebooks in Google Cloud Notebook, see the [Google Cloud Notebook guide](https://cloud.google.com/vertex-ai/docs/general/notebooks).\n",
    "\n",
    "This notebook demonstrate how to create a Custom Model using Custom Python Package Training, and how to serve the model using a pre-built scikit-learn container for online prediction, and batch prediction. It will require you provide a bucket where the dataset will be stored.\n",
    "\n",
    "**This example uses prebuilt containers for running predictions**, click [here](https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform/notebooks/unofficial/AI_Platform_Custom_Container_Prediction_sklearn.ipynb) for an example on how to build your own container for predictions using scikit-learn.\n",
    "\n",
    "Note: you may incur charges for training, prediction, storage or usage of other GCP products in connection with testing this SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Vertex SDK for Python\n",
    "\n",
    "\n",
    "After the SDK installation the kernel will be automatically restarted. **In general, restart the Kernel once you finish installations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 uninstall -y google-cloud-aiplatform\n",
    "!pip3 install google-cloud-aiplatform\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install -r ./requirements.txt --user\n",
    "# !python3 -m pip install kfp==1.6.2\n",
    "# !python3 -m pip install google-cloud-aiplatform\n",
    "# !python3 -m pip install google-cloud-storage==1.32\n",
    "# !python3 -m pip install build\n",
    "# !gcloud components update --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restart the Kernel once you finished installations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from googleapiclient import discovery\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List your current GCP project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = !gcloud config list --format 'value(core.project)' 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure your system variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your global variables\n",
    "PROJECT = project_id[0]          # Replace with your project ID\n",
    "USER = 'test_user'               # Replace with your user name\n",
    "BUCKET_NAME = project_id[0] + '-vertex-ai'       # Replace with your gcs bucket name\n",
    "\n",
    "FOLDER_NAME = 'sklearn_models'\n",
    "ALGORITHM = 'isolation_forest'\n",
    "TIMEZONE = 'US/Pacific'         \n",
    "REGION = 'us-central1'           # bucket should be in same region as Vertex AI         \n",
    "PACKAGE_URIS = f\"gs://{BUCKET_NAME}/trainer/{FOLDER_NAME}/{ALGORITHM}/trainer-0.1.tar.gz\" \n",
    "TRAIN_FEATURE_PATH = f\"gs://{BUCKET_NAME}/{FOLDER_NAME}_data/{ALGORITHM}/train/train.csv\"\n",
    "TEST_FEATURE_PATH = f\"gs://{BUCKET_NAME}/{FOLDER_NAME}_data/{ALGORITHM}/test/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Project:      {PROJECT}\")\n",
    "print(f\"Bucket Name: {BUCKET_NAME}\")\n",
    "print(f\"Python Package URI: {PACKAGE_URIS}\")\n",
    "print(f\"Training Data URI:  {TRAIN_FEATURE_PATH}\")\n",
    "print(f\"Test Data URI:      {TEST_FEATURE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_TYPE = \"custom-py-pkg\"\n",
    "TASK_NAME = f\"{TASK_TYPE}\"\n",
    "TASK_DIR = f\"./{TASK_NAME}\"\n",
    "DATA_DIR = f\"{TASK_DIR}/data\"\n",
    "PYTHON_PACKAGE_APPLICATION_DIR = f\"{TASK_NAME}/trainer\"\n",
    "\n",
    "print(f\"Task Name:      {TASK_NAME}\")\n",
    "print(f\"Task Directory: {TASK_DIR}\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Python Package Directory: {PYTHON_PACKAGE_APPLICATION_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create your bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l $REGION gs://$BUCKET_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build python package and upload to your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $TASK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $TASK_DIR && python3 setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltr $TASK_DIR/dist/trainer-0.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp $TASK_DIR/dist/trainer-0.1.tar.gz $PACKAGE_URIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_source_train_url = TRAIN_FEATURE_PATH\n",
    "gcs_source_test_url = TEST_FEATURE_PATH\n",
    "local_source_train = DATA_DIR + \"/train/train.csv\"\n",
    "local_source_test = DATA_DIR + \"/test/test.csv\"\n",
    "\n",
    "print(f\"Train data content will be loaded to {gcs_source_train_url}\")\n",
    "print(f\"Local train data content is here {local_source_train}\")\n",
    "print(f\"Test data content will be loaded to {gcs_source_train_url}\")\n",
    "print(f\"Local test data content is here {local_source_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Generate train data\n",
    "x = 0.3 * rng.randn(100, 2)\n",
    "x_train = np.r_[x + 2, x - 2]\n",
    "# Generate some regular novel observations\n",
    "x = 0.3 * rng.randn(20, 2)\n",
    "x_test = np.r_[x + 2, x - 2]\n",
    "# Generate some abnormal novel observations\n",
    "x_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n",
    "\n",
    "if not os.path.exists(DATA_DIR + '/train'):\n",
    "    os.makedirs(DATA_DIR + '/train')\n",
    "if not os.path.exists(DATA_DIR + '/test'):\n",
    "    os.makedirs(DATA_DIR + '/test')\n",
    "np.savetxt(local_source_train, x_train, fmt='%s', delimiter=\",\")\n",
    "np.savetxt(local_source_test, x_test, fmt='%s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copy the dataset to GCS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp $local_source_train $gcs_source_train_url\n",
    "!gsutil cp $local_source_test $gcs_source_test_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"gs://{BUCKET_NAME}/{FOLDER_NAME}_data/{ALGORITHM}/train\"\n",
    "!gsutil ls $path\n",
    "path = f\"gs://{BUCKET_NAME}/{FOLDER_NAME}_data/{ALGORITHM}/test\"\n",
    "!gsutil ls $path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Training with Google Vertex AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full article, please visit: https://cloud.google.com/vertex-ai/docs\n",
    "\n",
    "Where Vertex AI fits in the ML workflow \\\n",
    "The diagram below gives a high-level overview of the stages in an ML workflow. The blue-filled boxes indicate where Vertex AI provides managed services and APIs:\n",
    "\n",
    "<img src=\"img/ml-workflow.svg\" alt=\"Drawing\">\n",
    "\n",
    "As the diagram indicates, you can use Vertex AI to manage the following stages in the ML workflow:\n",
    "\n",
    "- Train an ML model on your data:\n",
    " - Train model\n",
    " - Evaluate model accuracy\n",
    " - Tune hyperparameters\n",
    " \n",
    " \n",
    "- Deploy your trained model.\n",
    "\n",
    "- Send prediction requests to your model:\n",
    " - Online prediction\n",
    " - Batch prediction\n",
    " \n",
    " \n",
    "- Monitor the predictions on an ongoing basis.\n",
    "\n",
    "- Manage your models and model versions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train at local\n",
    "\n",
    "Before submitting training jobs to Vertex AI, you can test your train.py code in the local environment. You can test by running your python script in command line. This way you can make sure your your entire python package are ready to be submitted to the remote VMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the initial set of hyperparameters\n",
    "MAX_SAMPLES = '100'  # No of samples\n",
    "RANDOM_STATE_SEED = '42'\n",
    "\n",
    "# Train on local machine with python command\n",
    "!cd $TASK_DIR/trainer && ls\n",
    "!cd $TASK_DIR && python3 -m trainer.task \\\n",
    "    --input $TRAIN_FEATURE_PATH \\\n",
    "    --job-dir ./models \\\n",
    "    --max-samples $MAX_SAMPLES \\\n",
    "    --random-state-seed $RANDOM_STATE_SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the training feature and label path defined above\n",
    "print(\"TRAIN_FEATURE_PATH = \", TRAIN_FEATURE_PATH)\n",
    "\n",
    "# Vertex AI requires each job to have unique name, \n",
    "# Therefore, we use prefix + timestamp to form job names.\n",
    "JOB_NAME = 'sklearn_isolation_forest_train_{}_{}'.format(\n",
    "    USER,\n",
    "    datetime.now(timezone(TIMEZONE)).strftime(\"%m%d%y_%H%M\")\n",
    "    )\n",
    "# We use the job names as folder names to store outputs.\n",
    "JOB_DIR = 'gs://{}/{}/{}'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME,\n",
    "    JOB_NAME,\n",
    "    )\n",
    "\n",
    "print(\"JOB_NAME_TRN = \", JOB_NAME)\n",
    "print(\"JOB_DIR_TRN = \", JOB_DIR)\n",
    "\n",
    "MAX_SAMPLES = '100'  # No of samples\n",
    "RANDOM_STATE_SEED = '42'\n",
    "\n",
    "print(\"MAX_SAMPLES = \", MAX_SAMPLES)\n",
    "print(\"RANDOM_STATE_SEED = \", RANDOM_STATE_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor_image_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-3:latest'\n",
    "python_module = \"trainer.task\"\n",
    "api_endpoint = \"us-central1-aiplatform.googleapis.com\"\n",
    "machine_type = \"n1-standard-4\"\n",
    "        \n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "# Initialize client that will be used to create and send requests.\n",
    "# This client only needs to be created once, and can be reused for multiple requests.\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "custom_job = {\n",
    "    \"display_name\": JOB_NAME,\n",
    "    \"job_spec\": {\n",
    "        \"worker_pool_specs\": [\n",
    "            {\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": machine_type,\n",
    "                },\n",
    "                \"replica_count\": 1,\n",
    "                \"python_package_spec\": {\n",
    "                    \"executor_image_uri\": executor_image_uri,\n",
    "                    \"package_uris\": [PACKAGE_URIS],\n",
    "                    \"python_module\": python_module,\n",
    "                    \"args\": [\n",
    "                      '--input',\n",
    "                      TRAIN_FEATURE_PATH,\n",
    "                      '--job-dir',\n",
    "                      JOB_DIR,\n",
    "                      '--max-samples',\n",
    "                      MAX_SAMPLES,\n",
    "                      '--random-state-seed',\n",
    "                      RANDOM_STATE_SEED\n",
    "                    ],\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "parent = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "print(\"response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The training job will take about 10-15 minutes to complete.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the training job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the training job status\n",
    "job_id_trn = response.name.split('/')[-1]\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "name = client.custom_job_path(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    custom_job=job_id_trn,\n",
    ")\n",
    "response = client.get_custom_job(name=name)\n",
    "print(response.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "### Deploy the Model\n",
    "\n",
    "Vertex AI provides tools to upload your trained ML model to the cloud, so that you can send prediction requests to the model.\n",
    "\n",
    "In order to deploy your trained model on Vertex AI, you must save your trained model using the tools provided by your machine learning framework. This involves serializing the information that represents your trained model into a file which you can deploy for prediction in the cloud.\n",
    "\n",
    "Then you upload the saved model to a Cloud Storage bucket, and create a model resource on Vertex AI, specifying the Cloud Storage path to your saved model.\n",
    "\n",
    "When you deploy your model, you can also provide custom code (beta) to customize how it handles prediction requests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import model artifacts to Vertex AI \n",
    "\n",
    "When you import a model, you associate it with a container for Vertex AI to run prediction requests. You can use pre-built containers provided by Vertex AI, or use your own custom containers that you build and push to Container Registry or Artifact Registry.\n",
    "\n",
    "You can use a pre-built container if your model meets the following requirements:\n",
    "\n",
    "- Trained in Python 3.7 or later\n",
    "- Trained using TensorFlow, scikit-learn, or XGBoost\n",
    "- Exported to meet framework-specific requirements for one of the pre-built prediction containers\n",
    "\n",
    "The link to the list of pre-built predict container images:\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"scikit_isolation_forest_model\"\n",
    "\n",
    "response = aiplatform.Model.upload(\n",
    "    display_name = MODEL_NAME,\n",
    "    serving_container_image_uri = 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest',\n",
    "    artifact_uri = JOB_DIR\n",
    ")\n",
    "\n",
    "model_id = response.name.split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint\n",
    "\n",
    "You need the endpoint ID to deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ENDPOINT_DISPLAY_NAME = \"scikit_isolation_forest_model_endpoint\"\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=MODEL_ENDPOINT_DISPLAY_NAME, project=PROJECT, location=REGION,\n",
    ")\n",
    "\n",
    "endpoint_id = endpoint.resource_name.split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy Model to the endpoint\n",
    "\n",
    "You must deploy a model to an endpoint before that model can be used to serve online predictions; deploying a model associates physical resources with the model so it can serve online predictions with low latency. An undeployed model can serve batch predictions, which do not have the same low latency requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"scikit_isolation_forest_model\"\n",
    "DEPLOYED_MODEL_DISPLAY_NAME = \"scikit_isolation_forest_model_deployed\"\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "model = aiplatform.Model(model_name=model_id)\n",
    "\n",
    "# The explanation_metadata and explanation_parameters should only be\n",
    "# provided for a custom trained model and not an AutoML model.\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=DEPLOYED_MODEL_DISPLAY_NAME,\n",
    "    machine_type = \"n1-standard-4\",\n",
    "    sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Send inference requests to your model\n",
    "\n",
    "Vertex AI provides the services you need to request predictions from your model in the cloud.\n",
    "\n",
    "There are two ways to get predictions from trained models: online prediction (sometimes called HTTP prediction) and batch prediction. In both cases, you pass input data to a cloud-hosted machine-learning model and get inferences for each data instance.\n",
    "\n",
    "Vertex AI online prediction is a service optimized to run your data through hosted models with as little latency as possible. You send small batches of data to the service and it returns your predictions in the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Google API for online inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import errors\n",
    "import pandas as pd\n",
    "\n",
    "# Load test feature and labels\n",
    "x_test = pd.read_csv(TEST_FEATURE_PATH)\n",
    "\n",
    "# Fill nan value with zeros (Prediction lacks the ability to handle nan values for now)\n",
    "x_test = x_test.fillna(0)\n",
    "\n",
    "pprobas = []\n",
    "batch_size = 10\n",
    "n_samples = min(160,x_test.shape[0])\n",
    "print(\"batch_size=\", batch_size)\n",
    "print(\"n_samples=\", n_samples)\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "for i in range(0, n_samples, batch_size):\n",
    "    j = min(i+batch_size, n_samples)\n",
    "    print(\"Processing samples\", i, j)\n",
    "    response = aiplatform.Endpoint(endpoint_id).predict(instances=x_test.iloc[i:j].values.tolist())\n",
    "    try:\n",
    "        for prediction_ in response.predictions:\n",
    "            pprobas.append(prediction_)\n",
    "    except errors.HttpError as err:\n",
    "        # Something went wrong, print out some information.\n",
    "        tf.compat.v1.logging.error('There was an error getting the job info, Check the details:')\n",
    "        tf.compat.v1.logging.error(err._get_reason())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprobas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Google GCLOUD API for online inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test feature and labels\n",
    "x_test = pd.read_csv(TEST_FEATURE_PATH)\n",
    "\n",
    "# Fill nan value with zeros (Prediction lacks the ability to handle nan values for now)\n",
    "x_test = x_test.fillna(0)\n",
    "\n",
    "# Create a temporary json file to contain data to be predicted\n",
    "JSON_TEMP = 'test_data.json' # temp json file name to hold the inference data\n",
    "batch_size = 100                # data batch size\n",
    "start = 0\n",
    "ind = 0\n",
    "end = min(ind+batch_size, len(x_test))\n",
    "body={'instances': x_test.iloc[start:end].values.tolist()}\n",
    "with open(JSON_TEMP, 'w') as fp:\n",
    "    fp.write(json.dumps(body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai endpoints predict $endpoint_id \\\n",
    "  --region=$REGION \\\n",
    "  --json-request=$JSON_TEMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Prediction Job on the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "model_instances = aiplatform.Model.list(\n",
    "    filter='display_name=\"scikit_isolation_forest_model\"'\n",
    ")\n",
    "for resource in model_instances:\n",
    "    #print(dir(resource))\n",
    "    print(resource.display_name)\n",
    "    print(resource.resource_name)\n",
    "    model_name = resource.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_name)\n",
    "model = aiplatform.Model(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "\n",
    "x_test_from_csv = genfromtxt(local_source_test, delimiter=',')\n",
    "JSON_TEST = 'test_data_batch.json' # temp json file name to hold the inference data\n",
    "with open(JSON_TEST, 'w') as json_file:\n",
    "  for row in x_test_from_csv.tolist():\n",
    "    json.dump(row, json_file)\n",
    "    json_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_source_batch_json_url=f\"gs://{BUCKET_NAME}/{FOLDER_NAME}_data/{ALGORITHM}/batch/input/test_data_batch.json\"\n",
    "gcs_destination_batch_url=f\"gs://{BUCKET_NAME}/{FOLDER_NAME}_data/{ALGORITHM}/batch/output\"\n",
    "local_source_batch_json = \"test_data_batch.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp $local_source_batch_json $gcs_source_batch_json_url\n",
    "print(f\"Test data content is loaded to {gcs_source_batch_json_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $gcs_source_batch_json_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=f\"temp_{TASK_NAME}_isolation_forest-serving\",\n",
    "    gcs_source=gcs_source_batch_json_url,\n",
    "    gcs_destination_prefix=gcs_destination_batch_url,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predict_job.wait()\n",
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "\n",
    "prediction_errors_stats = list()\n",
    "prediction_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction.errors_stats\"):\n",
    "        prediction_errors_stats.append(blob.name)\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction.results\"):\n",
    "        prediction_results.append(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $gcs_destination_batch_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ‰ Congratulations! ðŸŽ‰\n",
    "\n",
    "You've learned how to use Vertex AI to:\n",
    "\n",
    "Train a model by providing the training code in a pre-built container. You used a scikit-learn model in this example, but you can train a model built with any framework using custom containers.\n",
    "Deploy a scikit-learn model using a pre-built container as part of the same workflow you used for training.\n",
    "Create a model endpoint and generate a prediction.\n",
    "Run batch predictions.\n",
    "To learn more about different parts of Vertex, check out the documentation.\n",
    "https://cloud.google.com/vertex-ai/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
